dataroot: '~/torchvision_data_dir' #torchvision data folder
logdir: '~/logdir'
plotsdir: null # use default subfolder in logdir
chkptdir: null # use default subfolder in logdir
seed: 42
enable_tb: True # if False then TensorBoard logging is ignored
horovod: False
checkpoint_freq: 10
report_freq: 1 # after every N updates dump loss and other metrics in logger
# TODO: workers setting

# reddis address of Ray cluster. Use None for single node run
# otherwise it should something like host:6379. Make sure to run on head node:
# "ray start --head --redis-port=6379"
redis: null
gpus: null # use GPU IDs specified here (comma separated), if null then use all GPUs

smoke_test: False
only_eval: False
resume: False

dataset:
  name: 'cifar10'
  n_classes: 10
  ch_in: 3 # number of channels in image
  max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)

autoaug:
  num_op: 2
  num_policy: 5
  num_search: 200
  num_result_per_cv: 10 # after conducting N trials, we will chose the results of top num_result_per_cv
  loader:
    aug: '' # additional augmentations to use
    cutout: 16 # cutout length, use cutout augmentation when > 0
    batch: 64
    epochs: 50
    val_ratio: 0.4 #split portion for test set, 0 to 1
    val_fold: 0 #Fold number to use (0 to 4)
    cv_num: 5 # total number of folds available
    n_workers: 4 # if null then gpu_count*4
  optimizer:
    type: 'sgd'
    decay: 0
    momentum: 0.9 # pytorch default is 0
    nesterov: False
    clip: 5. # grads above this value is clipped
    warmup: null
      # multiplier: 2
      # epochs: 3
    #betas: [0.9, 0.999] # PyTorch default betas for Adam
  lr_schedule:
    type: 'cosine'
    lr_min: 0.0 # min learning rate, this will be used in eta_min param of scheduler

darts:
  test:
    genotype: 'PT_DARTS' # original paper: 'DARTS'
    aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
    drop_path_prob: 0.2 # drop path probability
    model_filename: 'cifar10_model.pt'
    ch_out_init: 36 # num of channels for stem outpt node
    layers: 20 # total number of layers
    data_parallel: False
    model_class: 'Cifar10TestModel'
    train_lossfn:
      type: 'CrossEntropyLoss'
    test_lossfn:
      type: 'CrossEntropyLoss'
    loader:
      aug: '' # additional augmentations to use
      cutout: 16 # cutout length, use cutout augmentation when > 0
      batch: 96
      epochs: 600
      val_ratio: 0.4 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      n_workers: 4 # if null then gpu_count*4
    optimizer:
      type: 'sgd'
      lr: 0.025 # init learning rate
      decay: 3.0e-4
      momentum: 0.9 # pytorch default is 0
      nesterov: False
      clip: 5. # grads above this value is clipped
      warmup: null
    lr_schedule:
      type: 'cosine'
      lr_min: 0.001 # min learning rate to se bet in eta_min param of scheduler
  search:
    bilevel: True
    ch_out_init: 16 # num of channels for stem outpt node
    layers: 8 # total number of layers
    lossfn:
      type: 'CrossEntropyLoss'
    loader:
      aug: '' # additional augmentations to use
      cutout: 0 # cutout length, use cutout augmentation when > 0
      batch: 64
      epochs: 50
      val_ratio: 0.5 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      n_workers: 4 # if null then gpu_count*4
    alphas:
      optimizer:
        type: 'adam'
        lr: 3.0e-4
        decay: 1.0e-3
        betas: [0.5, 0.999]
      lr_schedule: null
      lossfn:
        type: 'CrossEntropyLoss'
    weights:
      optimizer:
        type: 'sgd'
        lr: 0.025 # init learning rate
        decay: 3.0e-4
        momentum: 0.9 # pytorch default is 0
        nesterov: False
        clip: 5. # grads above this value is clipped
        warmup: null
      lr_schedule:
        type: 'cosine'
        lr_min: 0.001 # min learning rate, this will be used in eta_min param of scheduler
      lossfn:
        type: 'CrossEntropyLoss'
