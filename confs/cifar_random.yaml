common:
    logdir: '~/logdir'
    plotsdir: null # use default subfolder in logdir
    chkptdir: null # use default subfolder in logdir
    seed: 42
    enable_tb: True # if False then TensorBoard logging is ignored
    horovod: False
    checkpoint_freq: 10
    logger_freq: 10 # after every N updates dump loss and other metrics in logger
    detect_anomaly: False # if True, PyTorch code will run 6X slower
    # TODO: workers setting

    # reddis address of Ray cluster. Use None for single node run
    # otherwise it should something like host:6379. Make sure to run on head node:
    # "ray start --head --redis-port=6379"
    redis: null
    gpus: null # use GPU IDs specified here (comma separated), if null then use all GPUs

    smoke_test: False
    only_eval: False
    resume: False
  
dataset:
    dataroot: '~/torchvision_data_dir' #torchvision data folder
    name: 'cifar10'
    n_classes: 10
    ch_in: 3 # number of channels in image
    max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)
  
  
random:
    test:
        model_desc_file: 'best_model_desc.yaml'
        model_file: 'cifar_model.pt'
        data_parallel: False
        model_desc:
            init_ch_out: 36 # num of channels for stem outpt node
            n_cells: 20 # number of cells
            n_nodes: 4 # number of nodes in a cell
            n_out_nodes: 4 # last n nodes to concate output from
            stem_multiplier: 3 # output channels multiplier for the stem
            aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
            drop_path_prob: 0.2 # probability that given edge will be dropped
        train_lossfn:
            type: 'CrossEntropyLoss'
        test_lossfn:
            type: 'CrossEntropyLoss'
        loader:
            aug: '' # additional augmentations to use
            cutout: 16 # cutout length, use cutout augmentation when > 0
            batch: 96
            epochs: 600
            val_ratio: 0.4 #split portion for test set, 0 to 1
            val_fold: 0 #Fold number to use (0 to 4)
            cv_num: 5 # total number of folds available
            n_workers: null # if null then gpu_count*4
        optimizer:
            type: 'sgd'
            lr: 0.025 # init learning rate
            decay: 3.0e-4
            momentum: 0.9 # pytorch default is 0
            nesterov: False
            clip: 5. # grads above this value is clipped
            warmup: null
        lr_schedule:
            type: 'cosine'
            lr_min: 0.001 # min learning rate to se bet in eta_min param of scheduler
    
    
    
          
  
  
        