dataroot: '~/data/ImageNet' #torchvision data folder
logdir: '~/logdir'
seed: 0
enable_tb: True # if False then TensorBoard logging is ignored
horovod: False
checkpoint_freq: 10
report_freq: 100 # after every N updates dump loss and other metrics in logger
# TODO: workers setting

# reddis address of Ray cluster. Use None for single node run
# otherwise it should something like host:6379. Make sure to run on head node:
# "ray start --head --redis-port=6379"
redis: null
gpus: null # use GPU IDs specified here (comma separated), if null then use all GPUs

smoke_test: False
only_eval: False
resume: False

dataset:
  name: 'imagenet'
  n_classes: 1000
  ch_in: 3 # number of channels in image

darts:
  test:
    genotype: 'DARTS'
    aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
    drop_path_prob: 0.0 # drop path probability
    model_filename: 'imagenet_model.pt'
    ch_out_init: 48 # num of channels for stem outpt node
    layers: 14 # total number of layers
    data_parallel: True
    test_lossfn:
      lossfn:
        type: 'CrossEntropyLabelSmooth'
        smoothing: 0.1
    loader:
      aug: '' # additional augmentations to use
      cutout: 16 # cutout length, use cutout augmentation when > 0
      batch: 128
      epochs: 250
      val_ratio: 0.4 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
    optimizer:
      type: 'sgd'
      lr: 0.1 # init learning rate
      decay: 3.0e-5
      momentum: 0.9 # pytorch default is 0
      nesterov: False
      clip: 5. # grads above this value is clipped
      warmup: null
    lr_schedule:
      type: 'step'
      decay_period: 1 # epochs between two learning rate decays
      gamma: 0.97 # learning rate decay
