common:
  logdir: "~/logdir"
  seed: 2
  enable_tb: True # if False then TensorBoard logging is ignored
  horovod: &horovod False
  device: &device 'cuda'
  checkpoint_freq: 10
  detect_anomaly: False # if True, PyTorch code will run 6X slower
  # TODO: workers setting

  # reddis address of Ray cluster. Use None for single node run
  # otherwise it should something like host:6379. Make sure to run on head node:
  # "ray start --head --redis-port=6379"
  redis: null
  gpus: null # use GPU IDs specified here (comma separated), if null then use all GPUs

  smoke_test: False
  only_eval: False
  resume: False

dataset: &dataset
  dataroot: "~/torchvision_data_dir" #torchvision data folder
  name: "cifar10"
  n_classes: 10
  channels: 3 # number of channels in image
  max_batches: -1 # if >= 0 then only these many batches are generated (useful for debugging)

nas:
  search:
    data_parallel: False
    model_desc_file: "best_model_desc.yaml" # found arch is saved in this file
    device: *device
    model_desc:
      init_ch_out: 16 # num of output channels for the first cell
      n_cells: 8 # number of cells
      n_nodes: 4 # number of nodes in a cell
      out_nodes: 4 # last n nodes to concate output from
      stem_multiplier: 3 # output channels multiplier for the stem
      aux_tower: False
      dataset: *dataset
    loader:
      aug: "" # additional augmentations to use
      cutout: 0 # cutout length, use cutout augmentation when > 0
      batch: 64
      val_ratio: 0.5 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      n_workers: null # if null then gpu_count*4
      load_train: True # load train split of dataset
      load_test: False # load test split of dataset
      horovod: *horovod
      dataset: *dataset
    trainer:
      aux_weight: 0.0 # weight for loss from auxiliary towers in test time arch
      drop_path_prob: 0.0 # probability that given edge will be dropped
      grad_clip: 5.0 # grads above this value is clipped
      logger_freq: 10 # after every N updates dump loss and other metrics in logger
      title: "search_train"
      epochs: 50
      # additional vals for the derived class
      max_final_edges: 2 # max edge that can be in final arch per node
      plotsdir: 'plots' # use default subfolder in logdir
      l1_alphas: 0.0   # weight to be applied to sum(abs(alpahas)) to loss term
      lossfn:
        type: "CrossEntropyLoss"
      optimizer:
        type: "sgd"
        lr: 0.025 # init learning rate
        decay: 3.0e-4
        momentum: 0.9 # pytorch default is 0
        nesterov: False
        warmup: null
      alpha_optimizer:
        type: "adam"
        lr: 3.0e-4
        decay: 1.0e-3
        betas: [0.5, 0.999]
      lr_schedule:
        type: "cosine"
        lr_min: 0.001 # min learning rate, this will be used in eta_min param of scheduler
      validation: null
  eval:
    model_desc_file: "best_model_desc.yaml"
    save_filename: "model.pt" # file to which trained model will be saved
    device: *device
    data_parallel: False
    model_desc:
      init_ch_out: 36 # num of channels for stem outpt node
      n_cells: 20 # number of cells
      n_nodes: 4 # number of nodes in a cell
      out_nodes: 4 # last n nodes to concate output from
      stem_multiplier: 3 # output channels multiplier for the stem
      aux_tower: True
      dataset: *dataset
    loader:
      aug: "" # additional augmentations to use
      cutout: 16 # cutout length, use cutout augmentation when > 0
      batch: 96
      val_ratio: 0.0 #split portion for test set, 0 to 1
      val_fold: 0 #Fold number to use (0 to 4)
      cv_num: 5 # total number of folds available
      n_workers: null # if null then gpu_count*4
      load_train: True # load train split of dataset
      load_test: True # load test split of dataset
      horovod: *horovod
      dataset: *dataset
    trainer:
      aux_weight: 0.4 # weight for loss from auxiliary towers in test time arch
      drop_path_prob: 0.2 # probability that given edge will be dropped
      grad_clip: 5.0 # grads above this value is clipped
      logger_freq: 10 # after every N updates dump loss and other metrics in logger
      title: "eval_train"
      epochs: 600
      lossfn:
        type: "CrossEntropyLoss"
      optimizer:
        type: "sgd"
        lr: 0.025 # init learning rate
        decay: 3.0e-4
        momentum: 0.9 # pytorch default is 0
        nesterov: False
        warmup: null
      lr_schedule:
        type: "cosine"
        lr_min: 0.001 # min learning rate to se bet in eta_min param of scheduler
      validation:
        title: "eval_test"
        logger_freq: 1000
        lossfn:
          type: "CrossEntropyLoss"


autoaug:
  num_op: 2
  num_policy: 5
  num_search: 200
  num_result_per_cv: 10 # after conducting N trials, we will chose the results of top num_result_per_cv
  loader:
    aug: "" # additional augmentations to use
    cutout: 16 # cutout length, use cutout augmentation when > 0
    batch: 64
    epochs: 50
    val_ratio: 0.4 #split portion for test set, 0 to 1
    val_fold: 0 #Fold number to use (0 to 4)
    cv_num: 5 # total number of folds available
    n_workers: null # if null then gpu_count*4
    horovod: *horovod
    dataset: *dataset
  optimizer:
    type: "sgd"
    decay: 0
    momentum: 0.9 # pytorch default is 0
    nesterov: False
    warmup:
      null
      # multiplier: 2
      # epochs: 3
    #betas: [0.9, 0.999] # PyTorch default betas for Adam
  lr_schedule:
    type: "cosine"
    lr_min: 0.0 # min learning rate, this will be used in eta_min param of scheduler

